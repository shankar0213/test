
def transform_rules(df):

    # Quote map notes for default rows
    df["Map Note"] = df.apply(
        lambda x: f"\"{x['Map Note']}\"" if x["Process Type"] == "Default" else x["Map Note"],
        axis=1
    )

    # Build the source for non-default rows
    df["Source"] = df.apply(
        lambda x: None if x["Process Type"] == "Default" 
        else f"{x['Source Object Name']}.{x['Source Attributes Name']}",
        axis=1
    )

    output_rows = []

    for rule, group in df.groupby("Rule Name"):

        if "Default" in group["Process Type"].values:
            # Only take the default row
            default_row = group[group["Process Type"] == "Default"].iloc[0]
            output_rows.append({
                "Rule Name": rule,
                "Map Note": default_row["Map Note"],
                "Source": None
            })

        else:
            # No default â†’ pivot all sources
            sources = "[" + ", ".join(group["Source"].dropna().unique()) + "]"
            output_rows.append({
                "Rule Name": rule,
                "Map Note": group.iloc[0]["Map Note"],
                "Source": sources
            })

    return pd.DataFrame(output_rows)



col 1 -> Rule Name (without Duplicates)
col 2 -> Map Note (Check the column Process Type= "Default" the value in Map Note should come with double quotes "Map Note Value" and Source column value should be null else perform column operation )
col 3 -> Source (Cobination of , if more than one Rule Name all values of colum Source Object Name.Source Object Name should come inside [Source Object Name.Source Object Name] simplly have to be pivot)

 Data frame consists of column(Process Type,Rule Name,Source Object Name,Source Attributes Name,Map Note)
 out put dataframe needs ,(Rule Name, Map Note, Source)

import numpy as np

df["query"] = np.where(
    df["data_processing_rule"].str.strip() != "", 
    df["data_processing_rule"] + " AS " + df["target_attribute_name"],
    df["source_object_name"] + "." + df["source_attribute_name"] + " AS " + df["target_attribute_name"]
)


Subject: Clarification Required on Tokenization Handling for prty_srogt_id in c_aprvd_lmt Table
Hi Team,
We have identified an issue related to the prty_srogt_id column in the c_aprvd_lmt core table. This table is built using the combination of two map group IDs (i.e., it derives values from two different source tables).
In the current setup:
From Table 1, the relationship_no column is tokenised.
From Table 2, the relationship_no column is not tokenised.
Since the target column prty_srogt_id is populated using values coming from both tables, the resulting data is a mix of tokenised and non-tokenised values. During YAML development, when we attempt to un-tokenise this column, we need clarity on how this should be handled.
Request for clarification:
Should we perform un-tokenisation only on the specific tokenised values coming from Table 1?
OR
Should the relationship_no column in Table 2 also be tokenised for consistency, so that the target column contains only uniformly tokenised data?
Please confirm the expected approach so we can proceed accordingly.


subject:%20Clarification%20Required%20on%20Tokenization%20Handling%20for%20prty_srogt_id%20in%20c_aprvd_lmt%20Table%0AHi%20Team,%0AWe%20have%20identified%20an%20issue%20related%20to%20the%20prty_srogt_id%20column%20in%20the%20c_aprvd_lmt%20core%20table.%20This%20table%20is%20built%20using%20the%20combination%20of%20two%20map%20group%20IDs%20(i.e.,%20it%20derives%20values%20from%20two%20different%20source%20tables).%0AIn%20the%20current%20setup%3A%0AFrom%20Table%201,%20the%20relationship_no%20column%20is%20tokenised.%0AFrom%20Table%202,%20the%20relationship_no%20column%20is%20not%20tokenised.%0ASince%20the%20target%20column%20prty_srogt_id%20is%20populated%20using%20values%20coming%20from%20both%20tables,%20the%20resulting%20data%20is%20a%20mix%20of%20tokenised%20and%20non-tokenised%20values.%20During%20YAML%20development,%20when%20we%20attempt%20to%20un-tokenise%20this%20column,%20we%20need%20clarity%20on%20how%20this%20should%20be%20handled.%0ARequest%20for%20clarification%3A%0AShould%20we%20perform%20un-tokenisation%20only%20on%20the%20specific%20tokenised%20values%20coming%20from%20Table%201?%0AOR%0AShould%20the%20relationship_no%20column%20in%20Table%202%20also%20be%20tokenised%20for%20consistency,%20so%20that%20the%20target%20column%20contains%20only%20uniformly%20tokenised%20data?%0APlease%20confirm%20the%20expected%20approach%20so%20we%20can%20proceed%20accordingly.


Core and ADM HQL development has been completed, and weâ€™ve sent an email to Priya and Saraswathi for testing.
YAML creation is in progress, and we are concurrently updating the YAML files based on the latest mapping sheets.
The security matrix for naen views has been approved today, and we will start the YAML testing.


SELECT COUNT(*) 
FROM (
    SELECT
        WEP.PRTY_SROGT_ID AS PRTY_SROGT_ID,
        WEP.DOM_PRTY_REF AS DOM_PRTY_REF,
        CASE 
            WHEN TRIM(WEP.PROFILE_ID) IS NOT NULL AND TRIM(WEP.PROFILE_ID) != '' 
                THEN CONCAT(WEP.DATA_SRC_ICM, '-', WEP.SOURCE_COUNTRY_CODE_ICM, '-', WEP.PSEUDO_KEY)
            WHEN TRIM(WEP.PROFILE_ID) IS NULL OR TRIM(WEP.PROFILE_ID) = '' 
                THEN NULL 
        END AS ENTRP_PRTY_SROGT_ID,

        CASE 
            WHEN TRIM(WEP.PROFILE_ID) IS NOT NULL AND TRIM(WEP.PROFILE_ID) != '' 
                THEN WEP.PROFILE_ID
            ELSE NULL
        END AS ENTRP_PRTY_REF,

        CASE 
            WHEN WEP.PRTY_TYPE_CD = 'I' THEN NULL
            WHEN WEP.PRTY_TYPE_CD <> 'I' THEN 
                CASE 
                    WHEN (WEP.DOM_MAST_REF IS NULL OR WEP.DOM_MAST_REF = '') THEN NULL
                    WHEN WEP.DOM_MAST_REF IS NOT NULL THEN 
                        CASE 
                            WHEN TRIM(WEP.CLIENTID) IS NOT NULL AND TRIM(WEP.CLIENTID) != '' 
                                THEN CONCAT('DCB', '-', 'XX-', TRIM(WEP.CLIENTID)) 
                            WHEN TRIM(WEP.CLIENTID) IS NULL OR TRIM(WEP.CLIENTID) = '' 
                                THEN CONCAT('SCI', '-', 'XX-', TRIM(WEP.LE_ID)) 
                            ELSE NULL
                        END
                    ELSE NULL
                END
            ELSE NULL 
        END AS ENTRP_PARNT_PRTY_SROGT_ID,

        CASE 
            WHEN WEP.PRTY_TYPE_CD = 'I' THEN NULL
            WHEN WEP.PRTY_TYPE_CD <> 'I' THEN 
                CASE 
                    WHEN (WEP.DOM_MAST_REF IS NULL OR WEP.DOM_MAST_REF = '') THEN NULL
                    WHEN WEP.DOM_MAST_REF IS NOT NULL THEN TRIM(WEP.CLIENTID)
                    WHEN (TRIM(WEP.CLIENTID) IS NULL OR TRIM(WEP.CLIENTID) = '') 
                         AND (TRIM(WEP.LE_ID) IS NOT NULL AND TRIM(WEP.LE_ID) != '') 
                        THEN CONCAT(TRIM(WEP.LE_ID), '-', TRIM(WEP.SP_ID))
                    ELSE NULL 
                END
            ELSE NULL
        END AS ENTRP_PARNT_PRTY_HOST_NUM,

        TRIM(WEP.DOM_MAST_REF) AS DOM_MAST_REF,

        CASE 
            WHEN TRIM(WEP.PROFILE_ID) IS NOT NULL AND TRIM(WEP.PROFILE_ID) != '' 
                 AND (TRIM(WEP.LE_ID) IS NULL OR TRIM(WEP.LE_ID) = '') 
                THEN NULL
            WHEN TRIM(WEP.PROFILE_ID) IS NOT NULL AND TRIM(WEP.PROFILE_ID) != '' 
                 AND (TRIM(WEP.LE_ID) IS NOT NULL AND TRIM(WEP.LE_ID) != '') 
                THEN '-1i'
            WHEN (TRIM(WEP.PROFILE_ID) IS NULL OR TRIM(WEP.PROFILE_ID) = '') 
                 AND (TRIM(WEP.LE_ID) IS NOT NULL AND TRIM(WEP.LE_ID) != '') 
                THEN '-2i'
            WHEN (TRIM(WEP.PROFILE_ID) IS NULL OR TRIM(WEP.PROFILE_ID) = '') 
                 AND (TRIM(WEP.LE_ID) IS NULL OR TRIM(WEP.LE_ID) = '') 
                THEN '-3i'
        END AS MAST_PATN_TP_CD,

        CASE 
            WHEN TRIM(WEP.PROFILE_ID) IS NOT NULL AND TRIM(WEP.PROFILE_ID) != '' 
                 AND (TRIM(WEP.LE_ID) IS NULL OR TRIM(WEP.LE_ID) = '')  
                THEN NULL
            WHEN TRIM(WEP.PROFILE_ID) IS NOT NULL AND TRIM(WEP.PROFILE_ID) != '' 
                 AND (TRIM(WEP.LE_ID) IS NOT NULL AND TRIM(WEP.LE_ID) != '')  
                THEN 'Individual RELATIONSHIPNO with both SCI and ICM ID Exist'
            WHEN (TRIM(WEP.PROFILE_ID) IS NULL OR TRIM(WEP.PROFILE_ID) = '') 
                 AND (TRIM(WEP.LE_ID) IS NOT NULL AND TRIM(WEP.LE_ID) != '')  
                THEN 'Individual RELATIONSHIPNO with ICM ID not available but SCI ID available'
            WHEN (TRIM(WEP.PROFILE_ID) IS NULL OR TRIM(WEP.PROFILE_ID) = '') 
                 AND (TRIM(WEP.LE_ID) IS NULL OR TRIM(WEP.LE_ID) = '') 
                THEN 'Individual RELATIONSHIPNO with both ICM and SCI ID not available'
        END AS MAST_PATN_TP_DESC,

        WEP.INST_CLAS_CD AS INST_CLAS_CD,
        WEP.INST_CLAS_DESC AS INST_CLAS_DESC,
        WEP.SCB_STAF_FL AS SCB_STAF_FL,
        WEP.ISIC_CD AS ISIC_CD,
        WEP.ISIC_DESC AS ISIC_DESC,
        WEP.SEGMT_EFF_DT AS SEGMT_EFF_DT,
        '@DATA_SRC_EBB@' AS DATA_SRC,
        '@SOURCE_COUNTRY_CODE@' AS SOURCE_COUNTRY_CODE,
        '@ACCS_CTRY_CD@' AS ACCS_CTRY_CD,

        CASE 
            WHEN (TRIM(WEP.DOM_MAST_REF) IS NULL OR TRIM(WEP.DOM_MAST_REF) = '') THEN NULL
            WHEN TRIM(WEP.DOM_MAST_REF) IS NOT NULL AND TRIM(WEP.DOM_MAST_REF) != '' THEN PSE.ACCS_SEGMT_CD_PSEUDO 
            WHEN (TRIM(WEP.LE_ID) IS NULL OR TRIM(WEP.LE_ID) = '') THEN NULL
            ELSE NULL 
        END AS ACCS_SEGMT_CD,

        'EBB~ID~W_ENTRP_PRTY~C_CRTD_CORE_PRTY~1' AS PROCESS_ID

    FROM 
        (
            SELECT * 
            FROM (
                SELECT 
                    *, 
                    ROW_NUMBER() OVER (
                        PARTITION BY PRTY_SROGT_ID 
                        ORDER BY MAST_OPEN_DT DESC,
                            CASE 
                                WHEN (TRIM(LE_ID) IS NULL OR TRIM(LE_ID) = '') THEN 0 
                                ELSE 1 
                            END DESC
                    ) AS ROWNUMBER
                FROM UAT1_TB_DHUB_PRIM_WORK.W_ENTRP_PRTY  
                WHERE PROCESS_DATE = '20241231' 
                  AND DATA_SRC = 'EBB' 
                  AND SOURCE_COUNTRY_CODE = 'ID'
                  AND PRTY_TYPE_CD <> 'I'
            ) t
            WHERE t.ROWNUMBER = 1
        ) WEP
        LEFT JOIN (
            SELECT * 
            FROM UAT1_TB_DHUB_PRIM_WORK.C_PSEUDO_PRTY 
            WHERE DATA_SRC = 'SCI' AND SOURCE_COUNTRY_CODE = 'ID'
        ) PSC ON CONCAT(TRIM(WEP.LE_ID), '-', TRIM(WEP.SP_ID)) = PSC.SRC_KEY1
        LEFT JOIN (
            SELECT * 
            FROM UAT1_TB_DHUB_PRIM_WORK.C_PSEUDO_PRTY 
            WHERE DATA_SRC = 'ICM'
        ) PSI ON TRIM(WEP.PROFILE_ID) = TRIM(PSI.SRC_KEY2)
              AND WEP.SOURCE_COUNTRY_CODE = PSI.SOURCE_COUNTRY_CODE
) temp;

------------------------------
import paramiko
import pandas as pd
import numpy as np
import re

# Function to establish SSH connection
def establish_ssh_connection(host, user, password, port=22):
    try:
        ssh_client = paramiko.SSHClient()
        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh_client.connect(host, port=port, username=user, password=password)
        print("âœ… SSH Connection Established")
        return ssh_client
    except Exception as e:
        print(f"âŒ SSH connection failed: {e}")
        return None

# Function to run Beeline query and download result as CSV
def run_beeline_and_download_csv(ssh_client, query, remote_csv="/tmp/hive_output.csv", local_csv="hive_output.csv"):
    try:
        username = ssh_client.get_transport().get_username()
        query = f"set hive.mapred.mode=nonstrict; {query}"
        # Build the Beeline command
        beeline_cmd = f'beeline -n {username} --outputformat=csv2 -e "{query}" > {remote_csv}'

        # Run Beeline remotely
        stdin, stdout, stderr = ssh_client.exec_command(beeline_cmd)
        exit_status = stdout.channel.recv_exit_status()

        error = stderr.read().decode().strip()
        if exit_status != 0 or "ERROR" in error.upper():
            print(f"âš ï¸ Beeline failed:\n{error}")
            return None

        # Download the file via SFTP
        sftp = ssh_client.open_sftp()
        sftp.get(remote_csv, local_csv)
        sftp.remove(remote_csv)
        sftp.close()

        return local_csv

    except Exception as e:
        print(f"âŒ Error during Beeline or download: {e}")
        return None
        
        
# Data Cleaning Function
def data_cleaning(dataframe):
    dataframe.columns = [re.sub('[^a-zA-Z0-9]', '', names.lower()) for names in dataframe.columns.values]
    dataframe.fillna('no_value', inplace=True)
    dataframe = dataframe.astype("str")
    dataframe.sort_index(axis=1, inplace=True)
    dataframe.sort_values(by=list(dataframe.columns), inplace=True)
    dataframe.reset_index(inplace=True, drop=True)
    return dataframe

# Column Identification Function
def column_identification(df, variable):
    df_cols_to_drop = []
    for i in df.columns:
        k = re.sub('[^a-zA-Z0-9]', '', i.lower())
        for j in variable:
            j = re.sub('[^a-zA-Z0-9]', '', j.lower())
            if k == j:
                df_cols_to_drop.append(i)
    return df_cols_to_drop

    

# Main script logic
def main():
    print("ðŸ” SSH to Hive Server")

    # --- SSH DETAILS ---
    #
    ssh_host = input("Hostname/IP: ").strip()
    ssh_user = input("Username: ").strip()
    ssh_password = input("Password: ").strip()
    ssh_port = 22  # Or customize if needed

    # Connect via SSH
    ssh_client = establish_ssh_connection(ssh_host, ssh_user, ssh_password, ssh_port)
    if not ssh_client:
        return

    # --- QUERY1 INPUT ---
    print("\nðŸ“¥ Enter your actual table query (end with a semicolon `;`):")
    query = input().strip()

    # Run Beeline and get CSV
    local_csv_path = run_beeline_and_download_csv(ssh_client, query)

    # Load into pandas DataFrame
    if local_csv_path:
        try:
            df2 = pd.read_csv(local_csv_path)
        except Exception as e:
            print(f"âŒ Failed to read CSV: {e}")
            
    # --- QUERY2 INPUT ---
    print("\nðŸ“¥ Enter your test table query (end with a semicolon `;`):")
    query = input().strip()

    # Run Beeline and get CSV
    local_csv_path = run_beeline_and_download_csv(ssh_client, query)

    # Load into pandas DataFrame
    if local_csv_path:
        try:
            df1 = pd.read_csv(local_csv_path)
        except Exception as e:
            print(f"âŒ Failed to read CSV: {e}")        

    # Close SSH
    ssh_client.close()
    print("ðŸ”’ SSH connection closed.")
    
    
    ########################################
    my_variable_1 = 'Yes' #'row_col_count' 'No'
    my_variable_2 = [] #'columntodrop', [])
    my_variable_3 = [] #'columntoretain', [])
    #
    var2 = input("Enter columns to skip (separated by comma ','): ")
    if var2.strip() == "":
        word_list = []
    else:
        word_list2 = [word.strip() for word in var2.split(',')]
        my_variable_2 = word_list2
        
    var3 = input("Enter columns to reatin (separated by comma ','): ")    
    if var3.strip() == "":
        word_list = []
    else:
        word_list3 = [word.strip() for word in var3.split(',')]    
        my_variable_3 = word_list3
    
    
    #
    print("\nðŸ“Š Performing Validation....\n")
    
    print("Skipped Columns  = ",my_variable_2)
    print("Retained Columns = ",my_variable_3)
    
    DFDSS = pd.DataFrame(df1) #test
    DFPAX = pd.DataFrame(df2) #actual
    
    # Drop specified columns
    if len(my_variable_2) > 0:
        dfdss_cols_to_drop = column_identification(DFDSS, my_variable_2)
        dfpax_cols_to_drop = column_identification(DFPAX, my_variable_2)

        DFDSS = DFDSS.drop(dfdss_cols_to_drop, axis=1)
        DFPAX = DFPAX.drop(dfpax_cols_to_drop, axis=1)

    # Clean data
    DFDSS = data_cleaning(DFDSS)
    DFPAX = data_cleaning(DFPAX)

    # Normalize column names to lowercase and alphanumeric
    my_variable_3 = [re.sub('[^a-zA-Z0-9]', '', names.lower()) for names in my_variable_3]

    # Perform row and column count validation
    if my_variable_1 == 'Yes':
        if DFDSS.shape == DFPAX.shape and (DFPAX.columns == DFDSS.columns).sum() == DFPAX.shape[1]:
            if DFDSS.equals(DFPAX) == True:
                # If data is identical            
                result_df = pd.DataFrame({'Result': ['Both the datasets are the same.']})
                print(result_df)
                result_df.to_csv("validation_op.csv", index=False)
            else:
                # Data mismatch detected
                temp = pd.DataFrame(DFDSS == DFPAX)
                df1 = temp[~temp.all(axis=1)]
                
                for i in df1.columns:
                    if len(df1.index[df1[i] == False]) > 0:
                        error_df = DFDSS.iloc[df1.index[df1[i] == False]].loc[:,i:]
                        actual_df = DFPAX.iloc[df1.index[df1[i] == False]].loc[:,i:]

                        if len(my_variable_3) > 0:
                            retained_column = pd.DataFrame (DFDSS.iloc[df1.index[df1[i] == False]])[my_variable_3[0]]
                            
                        break

                new_column_name = actual_df.columns[0]
                error_df.insert(1, new_column_name +'_test', actual_df[new_column_name])

                if len(my_variable_3) > 0:
                    error_df.insert(0, my_variable_3[0]+ ' (primarykey)', retained_column)

                result_df = error_df
                result_df.to_csv("validation_op.csv", index=False)
                print(result_df)
        else:
            # Shape or column mismatch
            mismatch_df = pd.DataFrame()
            missing_paxata_col = []
            missing_dataiku_col = []

            if DFDSS.shape != DFPAX.shape:
                df_op = pd.DataFrame({'Result': ['There is a mismatch in actual, test row and column counts.'],'test Row/Column count': [DFDSS.shape],'actual Row/Column count': [DFPAX.shape]  })
                mismatch_df = mismatch_df._append(df_op)

            elif (DFPAX.columns == DFDSS.columns).sum() != DFPAX.shape[1] :
                for i in DFPAX.columns:
                    if i not in DFDSS.columns:
                        missing_paxata_col.append(i)

                for j in DFDSS.columns:
                    if j not in DFPAX. columns:
                        missing_dataiku_col.append(j)

                df_op2 = pd.DataFrame({'Result': ['There is mismatch in actual, test column names. '], 'Missing actual columns': [missing_paxata_col],'missing test columns':[missing_dataiku_col] })
                mismatch_df = mismatch_df._append (df_op2)

            
            result_df = mismatch_df
            result_df.to_csv("validation_op.csv", index=False)
            print(result_df)

    else:
        print("------------")
        if DFDSS.equals(DFPAX) == True:
            df = pd.DataFrame({'Result': ['Both the tables are same' ]})
            result_df = df
            result_df.to_csv("validation_op.csv", index=False)            # Compute a Pandas dataframe to write into OUTPUT DATASET NAME
            

        else:
            temp = pd.DataFrame(DFDSS == DFPAX)
            df1 = temp[~temp.a11(axis=1)]
            for i in df1.columns:
                if len(df1.index[df1[i] == False]) > 0:
                    error_df = DFDSS.iloc[df1.index[df1[i] == False]].loc[:, i:]
                    actual_df = DFPAX.iloc[df1.index[df1[i] == False]].loc[:, i:]

                    if len(my_variable_3) > 0:
                        retained_column = pd.DataFrame(DFDSS.iloc[df1.index[df1[i] == False]]) [my_variable_3[0]]

                    break
            new_column_name = actual_df.columns[0]
            error_df.insert (1, new_column_name+ '_test', actual_df[new_column_name])

            if len(my_variable_3) > 0:
                error_df.insert(0, my_variable_3[0]+ ' (primarykey)', retained_column)

            result_df = error_df
            result_df.to_csv("validation_op.csv", index=False)



        
    ########################################

if __name__ == "__main__":
    main()




--------------
py 11

PS C:\Users\bankid\Music> venv\Scripts\python.exe -c "import platform; print(platform.python_version()); print(platform.architecture())"


PS C:\Users\bankid\Music> py -3.11 -m venv venv
   
PS C:\Users\bankid\Music> venv\Scripts\pip.exe install pandas --trusted-host pypi.org --trusted-host files.pythonhosted.org --trusted-host pypi.python.org
PS C:\Users\bankid\Music> venv\Scripts\python.exe test1.py

