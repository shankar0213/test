
select count(*) from (SELECT
WEP.PRTY_SROGT_ID AS PRTY_SROGT_ID,
WEP.DOM_PRTY_REF AS DOM_PRTY_REF,
CASE WHEN trim(WPS.PROFILE_ID) IS NOT NULL  AND trim(WPS.PROFILE_ID) !='' THEN CONCAT (WPS.DATA_SRC_ICM, '-', WPS.SOURCE_COUNTRY_CODE_ICM, '-',WPS.PSEUDO_KEY)
	  WHEN (trim(WPS.PROFILE_ID) IS NULL OR trim(WPS.PROFILE_ID) ='') THEN NULL 
	  END AS ENTRP_PRTY_SROGT_ID,
	  
CASE WHEN  trim(WPS.PROFILE_ID) IS NOT NULL  AND trim(WPS.PROFILE_ID) !='' THEN WPS.PROFILE_ID
	  WHEN trim(WPS.PROFILE_ID) IS NULL OR trim(WPS.PROFILE_ID) ='' THEN NULL 
	  END AS ENTRP_PRTY_REF,
--cast(NULL as string) as ENTRP_PARNT_PRTY_SROGT_ID,
When WEP.PRTY_TYPE_CD = 'I', load null as ENTRP_PARNT_PRTY_SROGT_ID
When WEP.PRTY_TYPE_CD <> 'I',  
CASE WHEN (WEP.DOM_MAST_REF IS NULL  OR WEP.DOM_MAST_REF ="") THEN NULL
      WHEN WEP.DOM_MAST_REF IS NOT NULL THEN 
	  CASE WHEN (trim(WEP.CLIENTID) IS NOT NULL AND trim(DCB.CLIENTID) !='') THEN CONCAT('DCB','-','XX-',TRIM(WEP.CLIENTID)) 
	     WHEN (trim(WEP.CLIENTID) IS NULL OR trim(WEP.CLIENTID)='') THEN CONCAT('SCI','-','XX-', TRIM(WEP.LE_ID)) 
	  END
	  WHEN WEP.LE_ID IS NULL THEN NULL
	  ELSE NULL 
 	ELSE NULL END AS ENTRP_PARNT_PRTY_SROGT_ID

,When WEP.PRTY_TYPE_CD = 'I', load null as ENTRP_PARNT_PRTY_HOST_NUM
When WEP.PRTY_TYPE_CD <> 'I',  
,CASE WHEN (WEP.DOM_MAST_REF IS NULL  OR WEP.DOM_MAST_REF ="") THEN NULL
      WHEN WEP.DOM_MAST_REF IS NOT NULL THEN trim(WEP.CLIENTID)
        WHEN (trim(WEP.CLIENTID) IS NULL OR trim(WEP.CLIENTID)='') AND (trim(WEP.LE_ID) is not null  AND trim(WEP.LE_ID) !="" ) THEN CONCAT(TRIM(WEP.LE_ID), '-', TRIM(WEP.SP_ID))
              WHEN WEP.LE_ID IS NULL THEN NULL
              ELSE NULL END AS ENTRP_PARNT_PRTY_HOST_NUM

--cast(NULL as string) as ENTRP_PARNT_PRTY_HOST_NUM,
,trim(WPS.DOM_MAST_REF) AS DOM_MAST_REF,
CASE WHEN trim(WPS.PROFILE_ID) is not null AND trim(WPS.PROFILE_ID) !='' and (trim(WPS.LE_ID) is null or trim(WPS.LE_ID) ="" ) then null
		WHEN trim(WPS.PROFILE_ID) is not null AND trim(WPS.PROFILE_ID) !='' AND (trim(WPS.LE_ID) is not null AND trim(WPS.LE_ID) !="") then '-1i'
		WHEN (trim(WPS.PROFILE_ID) is null or  trim(WPS.PROFILE_ID) ='') and (trim(WPS.LE_ID) is not null AND trim(WPS.LE_ID) !="") then '-2i'
		WHEN (trim(WPS.PROFILE_ID) is null or  trim(WPS.PROFILE_ID) ='') and (trim(WPS.LE_ID) is null or trim(WPS.LE_ID) ="") then '-3i'
		END AS  MAST_PATN_TP_CD
,CASE WHEN trim(WPS.PROFILE_ID) is not null AND trim(WPS.PROFILE_ID) !='' AND (trim(WPS.LE_ID) is null or trim(WPS.LE_ID) ="" )  then null
		WHEN trim(WPS.PROFILE_ID) is not null  AND trim(WPS.PROFILE_ID) !='' AND (trim(WPS.LE_ID) is not null AND trim(WPS.LE_ID) !="") then 'Individual RELATIONSHIPNO with both SCI and ICM ID Exist'
		WHEN (trim(WPS.PROFILE_ID) is null or  trim(WPS.PROFILE_ID) ='') AND (trim(WPS.LE_ID) is not null AND trim(WPS.LE_ID) !="")  then 'Individual RELATIONSHIPNO with ICM ID not available but SCI ID available'
		WHEN (trim(WPS.PROFILE_ID) is null or  trim(WPS.PROFILE_ID) ='') AND (trim(WPS.LE_ID) is null or trim(WPS.LE_ID) ="") then 'Individual RELATIONSHIPNO with both ICM and SCI ID not available'
		END AS MAST_PATN_TP_DESC		
, WPS.INST_CLAS_CD AS INST_CLAS_CD
, WPS.INST_CLAS_DESC AS INST_CLAS_DESC
, WPS.SCB_STAF_FL AS SCB_STAF_FL
, WPS.ISIC_CD AS ISIC_CD
, WPS.ISIC_DESC AS ISIC_DESC
,WPS.SEGMT_EFF_DT as SEGMT_EFF_DT,
'EBB' as DATA_SRC,
'ID' as SOURCE_COUNTRY_CODE
,'ID' as ACCS_CTRY_CD
,CASE WHEN (trim(WEP.DOM_MAST_REF) IS NULL  OR trim(WEP.DOM_MAST_REF) ="")  THEN NULL
      WHEN trim(WEP.DOM_MAST_REF) IS NOT NULL AND trim(WEP.DOM_MAST_REF) !='' THEN PSE.ACCS_SEGMT_CD_PSEUDO 
	  WHEN (trim(WEP.LE_ID) is null or trim(WEP.LE_ID) ="" ) THEN NULL
	  ELSE NULL END AS ACCS_SEGMT_CD,
'EBB~ID~W_ENTRP_PRTY~C_CRTD_CORE_PRTY~1' as PROCESS_ID
FROM (SELECT  * FROM UAT1_TB_DHUB_PRIM_WORK.W_ENTRP_PRTY WHERE PRTY_TYPE_CD = 'I' AND PROCESS_DATE = '20241231' AND DATA_SRC = 'EBB' AND SOURCE_COUNTRY_CODE = 'ID')WEP

LEFT JOIN (SELECT * FROM UAT1_TB_DHUB_PRIM_WORK.C_PSEUDO_PRTY WHERE DATA_SRC = 'SCI' and SOURCE_COUNTRY_CODE = 'XX') PSC ON CONCAT(TRIM(WEP.LE_ID), '-', TRIM(WEP.SP_ID)) = PSC.SRC_KEY1 
LEFT JOIN (SELECT * FROM UAT1_TB_DHUB_PRIM_WORK.C_PSEUDO_PRTY WHERE DATA_SRC = 'ICM') PSI ON trim(WEP.PROFILE_ID) = trim(PSI.SRC_KEY2 ) and WEP.SOURCE_COUNTRY_CODE = PSI.SOURCE_COUNTRY_CODE)temp;

------------------------------
import paramiko
import pandas as pd
import numpy as np
import re

# Function to establish SSH connection
def establish_ssh_connection(host, user, password, port=22):
    try:
        ssh_client = paramiko.SSHClient()
        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh_client.connect(host, port=port, username=user, password=password)
        print("✅ SSH Connection Established")
        return ssh_client
    except Exception as e:
        print(f"❌ SSH connection failed: {e}")
        return None

# Function to run Beeline query and download result as CSV
def run_beeline_and_download_csv(ssh_client, query, remote_csv="/tmp/hive_output.csv", local_csv="hive_output.csv"):
    try:
        username = ssh_client.get_transport().get_username()
        query = f"set hive.mapred.mode=nonstrict; {query}"
        # Build the Beeline command
        beeline_cmd = f'beeline -n {username} --outputformat=csv2 -e "{query}" > {remote_csv}'

        # Run Beeline remotely
        stdin, stdout, stderr = ssh_client.exec_command(beeline_cmd)
        exit_status = stdout.channel.recv_exit_status()

        error = stderr.read().decode().strip()
        if exit_status != 0 or "ERROR" in error.upper():
            print(f"⚠️ Beeline failed:\n{error}")
            return None

        # Download the file via SFTP
        sftp = ssh_client.open_sftp()
        sftp.get(remote_csv, local_csv)
        sftp.remove(remote_csv)
        sftp.close()

        return local_csv

    except Exception as e:
        print(f"❌ Error during Beeline or download: {e}")
        return None
        
        
# Data Cleaning Function
def data_cleaning(dataframe):
    dataframe.columns = [re.sub('[^a-zA-Z0-9]', '', names.lower()) for names in dataframe.columns.values]
    dataframe.fillna('no_value', inplace=True)
    dataframe = dataframe.astype("str")
    dataframe.sort_index(axis=1, inplace=True)
    dataframe.sort_values(by=list(dataframe.columns), inplace=True)
    dataframe.reset_index(inplace=True, drop=True)
    return dataframe

# Column Identification Function
def column_identification(df, variable):
    df_cols_to_drop = []
    for i in df.columns:
        k = re.sub('[^a-zA-Z0-9]', '', i.lower())
        for j in variable:
            j = re.sub('[^a-zA-Z0-9]', '', j.lower())
            if k == j:
                df_cols_to_drop.append(i)
    return df_cols_to_drop

    

# Main script logic
def main():
    print("🔐 SSH to Hive Server")

    # --- SSH DETAILS ---
    #
    ssh_host = input("Hostname/IP: ").strip()
    ssh_user = input("Username: ").strip()
    ssh_password = input("Password: ").strip()
    ssh_port = 22  # Or customize if needed

    # Connect via SSH
    ssh_client = establish_ssh_connection(ssh_host, ssh_user, ssh_password, ssh_port)
    if not ssh_client:
        return

    # --- QUERY1 INPUT ---
    print("\n📥 Enter your actual table query (end with a semicolon `;`):")
    query = input().strip()

    # Run Beeline and get CSV
    local_csv_path = run_beeline_and_download_csv(ssh_client, query)

    # Load into pandas DataFrame
    if local_csv_path:
        try:
            df2 = pd.read_csv(local_csv_path)
        except Exception as e:
            print(f"❌ Failed to read CSV: {e}")
            
    # --- QUERY2 INPUT ---
    print("\n📥 Enter your test table query (end with a semicolon `;`):")
    query = input().strip()

    # Run Beeline and get CSV
    local_csv_path = run_beeline_and_download_csv(ssh_client, query)

    # Load into pandas DataFrame
    if local_csv_path:
        try:
            df1 = pd.read_csv(local_csv_path)
        except Exception as e:
            print(f"❌ Failed to read CSV: {e}")        

    # Close SSH
    ssh_client.close()
    print("🔒 SSH connection closed.")
    
    
    ########################################
    my_variable_1 = 'Yes' #'row_col_count' 'No'
    my_variable_2 = [] #'columntodrop', [])
    my_variable_3 = [] #'columntoretain', [])
    #
    var2 = input("Enter columns to skip (separated by comma ','): ")
    if var2.strip() == "":
        word_list = []
    else:
        word_list2 = [word.strip() for word in var2.split(',')]
        my_variable_2 = word_list2
        
    var3 = input("Enter columns to reatin (separated by comma ','): ")    
    if var3.strip() == "":
        word_list = []
    else:
        word_list3 = [word.strip() for word in var3.split(',')]    
        my_variable_3 = word_list3
    
    
    #
    print("\n📊 Performing Validation....\n")
    
    print("Skipped Columns  = ",my_variable_2)
    print("Retained Columns = ",my_variable_3)
    
    DFDSS = pd.DataFrame(df1) #test
    DFPAX = pd.DataFrame(df2) #actual
    
    # Drop specified columns
    if len(my_variable_2) > 0:
        dfdss_cols_to_drop = column_identification(DFDSS, my_variable_2)
        dfpax_cols_to_drop = column_identification(DFPAX, my_variable_2)

        DFDSS = DFDSS.drop(dfdss_cols_to_drop, axis=1)
        DFPAX = DFPAX.drop(dfpax_cols_to_drop, axis=1)

    # Clean data
    DFDSS = data_cleaning(DFDSS)
    DFPAX = data_cleaning(DFPAX)

    # Normalize column names to lowercase and alphanumeric
    my_variable_3 = [re.sub('[^a-zA-Z0-9]', '', names.lower()) for names in my_variable_3]

    # Perform row and column count validation
    if my_variable_1 == 'Yes':
        if DFDSS.shape == DFPAX.shape and (DFPAX.columns == DFDSS.columns).sum() == DFPAX.shape[1]:
            if DFDSS.equals(DFPAX) == True:
                # If data is identical            
                result_df = pd.DataFrame({'Result': ['Both the datasets are the same.']})
                print(result_df)
                result_df.to_csv("validation_op.csv", index=False)
            else:
                # Data mismatch detected
                temp = pd.DataFrame(DFDSS == DFPAX)
                df1 = temp[~temp.all(axis=1)]
                
                for i in df1.columns:
                    if len(df1.index[df1[i] == False]) > 0:
                        error_df = DFDSS.iloc[df1.index[df1[i] == False]].loc[:,i:]
                        actual_df = DFPAX.iloc[df1.index[df1[i] == False]].loc[:,i:]

                        if len(my_variable_3) > 0:
                            retained_column = pd.DataFrame (DFDSS.iloc[df1.index[df1[i] == False]])[my_variable_3[0]]
                            
                        break

                new_column_name = actual_df.columns[0]
                error_df.insert(1, new_column_name +'_test', actual_df[new_column_name])

                if len(my_variable_3) > 0:
                    error_df.insert(0, my_variable_3[0]+ ' (primarykey)', retained_column)

                result_df = error_df
                result_df.to_csv("validation_op.csv", index=False)
                print(result_df)
        else:
            # Shape or column mismatch
            mismatch_df = pd.DataFrame()
            missing_paxata_col = []
            missing_dataiku_col = []

            if DFDSS.shape != DFPAX.shape:
                df_op = pd.DataFrame({'Result': ['There is a mismatch in actual, test row and column counts.'],'test Row/Column count': [DFDSS.shape],'actual Row/Column count': [DFPAX.shape]  })
                mismatch_df = mismatch_df._append(df_op)

            elif (DFPAX.columns == DFDSS.columns).sum() != DFPAX.shape[1] :
                for i in DFPAX.columns:
                    if i not in DFDSS.columns:
                        missing_paxata_col.append(i)

                for j in DFDSS.columns:
                    if j not in DFPAX. columns:
                        missing_dataiku_col.append(j)

                df_op2 = pd.DataFrame({'Result': ['There is mismatch in actual, test column names. '], 'Missing actual columns': [missing_paxata_col],'missing test columns':[missing_dataiku_col] })
                mismatch_df = mismatch_df._append (df_op2)

            
            result_df = mismatch_df
            result_df.to_csv("validation_op.csv", index=False)
            print(result_df)

    else:
        print("------------")
        if DFDSS.equals(DFPAX) == True:
            df = pd.DataFrame({'Result': ['Both the tables are same' ]})
            result_df = df
            result_df.to_csv("validation_op.csv", index=False)            # Compute a Pandas dataframe to write into OUTPUT DATASET NAME
            

        else:
            temp = pd.DataFrame(DFDSS == DFPAX)
            df1 = temp[~temp.a11(axis=1)]
            for i in df1.columns:
                if len(df1.index[df1[i] == False]) > 0:
                    error_df = DFDSS.iloc[df1.index[df1[i] == False]].loc[:, i:]
                    actual_df = DFPAX.iloc[df1.index[df1[i] == False]].loc[:, i:]

                    if len(my_variable_3) > 0:
                        retained_column = pd.DataFrame(DFDSS.iloc[df1.index[df1[i] == False]]) [my_variable_3[0]]

                    break
            new_column_name = actual_df.columns[0]
            error_df.insert (1, new_column_name+ '_test', actual_df[new_column_name])

            if len(my_variable_3) > 0:
                error_df.insert(0, my_variable_3[0]+ ' (primarykey)', retained_column)

            result_df = error_df
            result_df.to_csv("validation_op.csv", index=False)



        
    ########################################

if __name__ == "__main__":
    main()




--------------
py 11

PS C:\Users\bankid\Music> venv\Scripts\python.exe -c "import platform; print(platform.python_version()); print(platform.architecture())"


PS C:\Users\bankid\Music> py -3.11 -m venv venv
   
PS C:\Users\bankid\Music> venv\Scripts\pip.exe install pandas --trusted-host pypi.org --trusted-host files.pythonhosted.org --trusted-host pypi.python.org
PS C:\Users\bankid\Music> venv\Scripts\python.exe test1.py

